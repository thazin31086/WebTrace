{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Skip-gram.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thazin31086/WebTrace/blob/master/Skip_gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2rISz83Yiwu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "6cfe7d01-d9e5-4fe1-f34f-a440b017a470"
      },
      "source": [
        "#https://towardsdatascience.com/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "corpus_raw = 'The cat sat on the mat'\n",
        "\n",
        "# convert to lower case\n",
        "corpus_raw = corpus_raw.lower()\n",
        "\n",
        "\n",
        "words = []\n",
        "for word in corpus_raw.split():   \n",
        "        words.append(word)\n",
        "words = set(words) # so that all duplicate words are removed\n",
        "\n",
        "word2int = {}\n",
        "int2word = {}\n",
        "vocab_size = len(words) # gives the total number of unique words\n",
        "print(vocab_size)\n",
        "for i,word in enumerate(words):\n",
        "    word2int[word] = i\n",
        "    int2word[i] = word\n",
        "\n",
        "    \n",
        " # raw sentences is a list of sentences.\n",
        "raw_sentences = corpus_raw.split('.')\n",
        "sentences = []\n",
        "for sentence in raw_sentences:\n",
        "    sentences.append(sentence.split())\n",
        "    \n",
        "print(sentences)\n",
        "\n",
        "\n",
        "data = []\n",
        "WINDOW_SIZE = 2\n",
        "for sentence in sentences:\n",
        "    for word_index, word in enumerate(sentence):\n",
        "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] : \n",
        "            if nb_word != word:\n",
        "                data.append([word, nb_word])\n",
        "                \n",
        "print(data)\n",
        "\n",
        "\n",
        "# function to convert numbers to one hot vectors\n",
        "def to_one_hot(data_point_index, vocab_size):\n",
        "    temp = np.zeros(vocab_size)\n",
        "    temp[data_point_index] = 1\n",
        "    return temp\n",
        "  \n",
        "x_train = [] # input word\n",
        "y_train = [] # output word\n",
        "for data_word in data:\n",
        "    x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))\n",
        "    y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))\n",
        "# convert them to numpy arrays\n",
        "x_train = np.asarray(x_train)\n",
        "y_train = np.asarray(y_train)\n",
        "\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "# meaning 18 training points, where each point has 5 dimensions\n",
        "\n",
        "# making placeholders for x_train and y_train\n",
        "x = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
        "y_label = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
        "\n",
        "\n",
        "EMBEDDING_DIM = vocab_size # you can choose your own number\n",
        "W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\n",
        "b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM])) #bias\n",
        "hidden_representation = tf.add(tf.matmul(x,W1), b1)\n",
        "\n",
        "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\n",
        "b2 = tf.Variable(tf.random_normal([vocab_size]))\n",
        "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))\n",
        "\n",
        "#to train it:\n",
        "sess = tf.Session()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init) #make sure you do this!\n",
        "# define the loss function:\n",
        "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), reduction_indices=[1]))\n",
        "# define the training step:\n",
        "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)\n",
        "n_iters = 10000\n",
        "# train for n_iter iterations\n",
        "for _ in range(n_iters):\n",
        "    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n",
        "    #rint('loss is : ', sess.run(cross_entropy_loss, feed_dict={x: x_train, y_label: y_train}))\n",
        "    \n",
        "vectors = sess.run(W1 + b1)\n",
        "print(vectors[ word2int['cat'] ])\n",
        "\n",
        "#quick function to find the closest vector to a given vector. Beware, itâ€™s a dirty implementation.\n",
        "def euclidean_dist(vec1, vec2):\n",
        "    return np.sqrt(np.sum((vec1-vec2)**2))\n",
        "def find_closest(word_index, vectors):\n",
        "    min_dist = 10000 # to act like positive infinity\n",
        "    min_index = -1\n",
        "    query_vector = vectors[word_index]\n",
        "    for index, vector in enumerate(vectors):\n",
        "        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n",
        "            min_dist = euclidean_dist(vector, query_vector)\n",
        "            min_index = index\n",
        "    return min_index\n",
        "  \n",
        "print(int2word[find_closest(word2int['cat'], vectors)])\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "[['the', 'cat', 'sat', 'on', 'the', 'mat']]\n",
            "[['the', 'cat'], ['the', 'sat'], ['cat', 'the'], ['cat', 'sat'], ['cat', 'on'], ['sat', 'the'], ['sat', 'cat'], ['sat', 'on'], ['sat', 'the'], ['on', 'cat'], ['on', 'sat'], ['on', 'the'], ['on', 'mat'], ['the', 'sat'], ['the', 'on'], ['the', 'mat'], ['mat', 'on'], ['mat', 'the']]\n",
            "(18, 5) (18, 5)\n",
            "[2.2158246 1.491334  0.9773514 2.0339558 1.3369646]\n",
            "mat\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}