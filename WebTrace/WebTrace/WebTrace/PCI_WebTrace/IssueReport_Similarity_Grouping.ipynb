{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IssueReport Similarity Grouping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TjhJuc9TAN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install wikipedia "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0xkQDXr8swL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install pyspellchecker"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUbjxCuq7Gcp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ce901a51-8214-4082-dc8d-b9507d5100dc"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import nltk\n",
        "import re\n",
        "import wikipedia\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from string import punctuation\n",
        "from nltk.stem import PorterStemmer\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "# tensroflow hub module for Universal sentence Encoder \n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" \n",
        "embed = hub.Module(module_url)\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def get_features(texts):\n",
        "    if type(texts) is str:\n",
        "        texts = [texts]\n",
        "    with tf.Session() as sess:\n",
        "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "        return sess.run(embed(texts))\n",
        "\n",
        "def remove_stopwords(stop_words, tokens):\n",
        "    res = []\n",
        "    for token in tokens:\n",
        "        if not token in stop_words:\n",
        "            res.append(token)\n",
        "    return res\n",
        "\n",
        "def process_text(text):\n",
        "    text = text.encode('ascii', errors='ignore').decode()\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', ' ', text)\n",
        "    text = re.sub(r'#+', ' ', text )\n",
        "    text = text.strip() #Remove white space from beginning and ending\n",
        "    return text\n",
        "\n",
        "def camel_case_split(tokens): \n",
        "   words = []\n",
        "   word_tokens = word_tokenize(tokens)\n",
        "   for token in word_tokens:\n",
        "        words = [[token[0]]]   \n",
        "        for c in tokens[1:]: \n",
        "            if words[-1][-1].islower() and c.isupper(): \n",
        "                words.append(list(c)) \n",
        "            else: \n",
        "                words[-1].append(c)   \n",
        "   return words\n",
        "\n",
        "def lemmatize(tokens):\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    lemma_list = []\n",
        "    word_tokens = word_tokenize(tokens) \n",
        "    for token in word_tokens:\n",
        "        lemma = lemmatizer.lemmatize(token, 'v')\n",
        "        if lemma == token:\n",
        "            lemma = lemmatizer.lemmatize(token)\n",
        "        lemma_list.append(lemma)  \n",
        "    return lemma_list\n",
        "\n",
        "def stemm(tokens):  \n",
        "  ps = PorterStemmer()\n",
        "  stem_list = []\n",
        "  word_tokens = word_tokenize(tokens) \n",
        "  for w in word_tokens:\n",
        "      rootWord = ps.stem(w)\n",
        "      stem_list.append(rootWord)\n",
        "  return stem_list\n",
        "\n",
        "def correct_spelling(tokens): \n",
        "    #print(tokens)\n",
        "    spell = SpellChecker()\n",
        "    spellchecked_list = []\n",
        "    # find those words that may be misspelled\n",
        "    word_tokens = word_tokenize(tokens) \n",
        "    for w in word_tokens:\n",
        "      alist = []\n",
        "      alist.append(w)\n",
        "      if len(spell.unknown(alist)) == 0:\n",
        "         spellchecked_list.append(w)\n",
        "      else:\n",
        "         spellchecked_list.append(wikisuggestion(w))\n",
        "    #print(spellchecked_list)\n",
        "    return spellchecked_list\n",
        "\n",
        "def wikisuggestion(token):\n",
        "    spell = SpellChecker()\n",
        "    wiki_list = wikipedia.search(token)\n",
        "    if len(wiki_list) == 0:  # No Suggested Word from Wiki, Correct Spelling with Python Spelling Checker\n",
        "        return spell.correction(token)\n",
        "    else:\n",
        "        for wl in wiki_list:\n",
        "           if wl in data: # Suggested the closest word based on the context. \n",
        "              return wl\n",
        "           else:\n",
        "              result = wikipedia.search(wl)[0]\n",
        "              result = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", result)\n",
        "              return result\n",
        "   \n",
        "def process_all(text):\n",
        "    text = process_text(text)\n",
        "    text = ' '.join(remove_stopwords(stop_words, text.split()))\n",
        "    #text = ' '.join(camel_case_split(text))\n",
        "    text = ' '.join(correct_spelling(text))\n",
        "    text = ' '.join(stemm(text))\n",
        "    text = ' '.join(lemmatize(text))\n",
        "    \n",
        "    return text\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    mag1 = np.linalg.norm(v1)\n",
        "    mag2 = np.linalg.norm(v2)\n",
        "    if (not mag1) or (not mag2):\n",
        "        return 0\n",
        "    return np.dot(v1, v2) / (mag1 * mag2)\n",
        "\n",
        "def test_similarity(text1, text2):\n",
        "    vec1 = get_features(text1)[0]\n",
        "    vec2 = get_features(text2)[0]\n",
        "    print(vec1.shape)\n",
        "    return cosine_similarity(vec1, vec2)\n",
        "\n",
        "def semantic_search(query, data, vectors):\n",
        "    query = process_all(query)\n",
        "    print(\"Extracting features...\")\n",
        "    query_vec = get_features(query)[0].ravel()\n",
        "    res = []\n",
        "    for i, d in enumerate(data):\n",
        "        qvec = vectors[i].ravel()\n",
        "        sim = cosine_similarity(query_vec, qvec)\n",
        "        res.append((sim, d[:100], i))\n",
        "    return sorted(res, key=lambda x : x[0], reverse=True)\n",
        "\n",
        "def unique_words(sentence):\n",
        "    return set(sentence.lower().split())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkOqVRTOVF-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [\n",
        "    \"Avoid crash on concat on structs with ToString member\", #https://github.com/dotnet/roslyn/pull/38860/commits\n",
        "    \"Enum implicit cast to string fails when element is named ToString\", #https://github.com/dotnet/roslyn/issues/40256\n",
        "    \"Enum with ToString member crashes in string concatenation\", #https://github.com/dotnet/roslyn/issues/38858\n",
        "    \"Visual Studio 2019 crashing when click RMB on rule in Analyzers' dependencies\",#https://github.com/dotnet/roslyn/issues/40720\n",
        "    \"Crash on right click a Analyze rule in Solution-Explorer\", #https://github.com/dotnet/roslyn/issues/36304\n",
        "    \"Handle lazy loading of analyzer command handlers\", #https://github.com/dotnet/roslyn/pull/36740\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M45sIQqVQ7j",
        "colab_type": "code",
        "outputId": "42504c53-b62b-4528-f42e-f33c143e1e38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "data_processed = list(map(process_all, data))\n",
        "BASE_VECTORS = get_features(data_processed)\n",
        "semantic_search(\"Handle lazy loading of analyzer command handler\", data_processed, BASE_VECTORS)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting features...\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1.0, 'handl lazi load analyz command handler', 5),\n",
              " (0.57930964, 'crash right click analyz rule solut', 4),\n",
              " (0.55621374, 'enumer type java member crash string concaten', 2),\n",
              " (0.5314942,\n",
              "  \"visual studio 2019 crash click renminbi rule spectrum analyz ' depend\",\n",
              "  3),\n",
              " (0.5302365, 'avoid crash concaten record java member', 0),\n",
              " (0.45067155, 'enumer type implicit cast string fail element name java', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTbuw0CtL1gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_processed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2je2iStGP0VI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "eb5cb971-1cd3-4521-cb31-3d9278d7fe26"
      },
      "source": [
        "semantic_search(\"Visual Studio 2019 crashing when click RMB on rule in Analyzers' dependencies\", data_processed, BASE_VECTORS)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "visual studio 2019 crashing click rmb rule analyzers' dependencies\n",
            "['visual', 'studio', '2019', 'crashing', 'click', 'Renminbi', 'rule', 'Spectrum analyzer', \"'\", 'dependencies']\n",
            "Extracting features...\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1.0000001,\n",
              "  \"visual studio 2019 crash click renminbi rule spectrum analyz ' depend\",\n",
              "  3),\n",
              " (0.7122575,\n",
              "  'avoid crash concaten record ( comput scienc ) java ( program languag ) member',\n",
              "  0),\n",
              " (0.63621086,\n",
              "  'enumer type java ( program languag ) member crash string concaten',\n",
              "  2),\n",
              " (0.6034917,\n",
              "  'enumer type implicit cast string fail element name java ( program languag )',\n",
              "  1),\n",
              " (0.58907, 'crash right click analyz rule solut', 4),\n",
              " (0.5314943, 'handl lazi load analyz command handler', 5)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOnVrJsZXgNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compare with Code\n",
        "issuereport_uniquewordlist = unique_words(' '.join(data[3:6]))\n",
        "issuereport_cluser1 = ' '.join(issuereport_uniquewordlist)\n",
        "codedata = ['project should other severity link get changed add item name items with hierarchy create checked show help initialize rule active context handler new folder enabled any controller menu diagnostics copy file update diagnostic analyzer open analyzers selected command remove set', \n",
        "            'Analyzer Command Handler Tests Traits Features Diagnostics Diagnostic Context Menu Controller Solution Explorer']\n",
        "codedata_processed = list(map(process_text, codedata))          \n",
        "code_BASE_VECTORS = get_features(codedata)\n",
        "\n",
        "print(issuereport_cluser1)\n",
        "semantic_search(issuereport_cluser1, codedata_processed, code_BASE_VECTORS)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}