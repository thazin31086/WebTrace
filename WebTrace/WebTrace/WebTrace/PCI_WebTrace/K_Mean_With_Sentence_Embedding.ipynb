{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "K_Mean_With_Sentence_Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hPp3mqH3y4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install wikipedia"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0fkVVf737Fk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install pyspellchecker"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE4Ow2Vw4Jkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import nltk\n",
        "import re\n",
        "import wikipedia\n",
        "import pandas as pd\n",
        "import datetime\n",
        "# import the math module  \n",
        "import math\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "nltk.download('brown')\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from string import punctuation\n",
        "from nltk.stem import PorterStemmer\n",
        "from spellchecker import SpellChecker\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "data = pd.read_csv('mono.csv', encoding='latin-1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uV3LFc84PiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tensroflow hub module for Universal sentence Encoder \n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" \n",
        "embed = hub.Module(module_url)\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def get_features(texts):\n",
        "    if type(texts) is str:\n",
        "        texts = [texts]\n",
        "    with tf.Session() as sess:\n",
        "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "        return sess.run(embed(texts))\n",
        "\n",
        "def remove_stopwords(stop_words, tokens):\n",
        "    res = []\n",
        "    for token in tokens:\n",
        "        if not token in stop_words:\n",
        "            res.append(token)\n",
        "    return res\n",
        "\n",
        "def process_text(text):\n",
        "    text = text.encode('ascii', errors='ignore').decode()\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', ' ', text)\n",
        "    text = re.sub(r'#+', ' ', text)\n",
        "    #text = re.sub(r'`\\S+', '', text)\n",
        "    #text = re.sub(r'.\\S+', ' ', text)\n",
        "    text = text.strip() #Remove white space from beginning and ending\n",
        "    return text\n",
        "\n",
        "def camel_case_split(tokens): \n",
        "   words = []\n",
        "   word_tokens = word_tokenize(tokens)\n",
        "   for token in word_tokens:\n",
        "        words = [[token[0]]]   \n",
        "        for c in tokens[1:]: \n",
        "            if words[-1][-1].islower() and c.isupper(): \n",
        "                words.append(list(c)) \n",
        "            else: \n",
        "                words[-1].append(c)   \n",
        "   return words\n",
        "\n",
        "def lemmatize(tokens):\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    lemma_list = []\n",
        "    word_tokens = word_tokenize(tokens) \n",
        "    for token in word_tokens:\n",
        "        lemma = lemmatizer.lemmatize(token, 'v')\n",
        "        if lemma == token:\n",
        "            lemma = lemmatizer.lemmatize(token)\n",
        "        lemma_list.append(lemma)  \n",
        "    return lemma_list\n",
        "\n",
        "def stemm(tokens):  \n",
        "  ps = PorterStemmer()\n",
        "  stem_list = []\n",
        "  word_tokens = word_tokenize(tokens) \n",
        "  for w in word_tokens:\n",
        "      rootWord = ps.stem(w)\n",
        "      stem_list.append(rootWord)\n",
        "  return stem_list\n",
        "\n",
        "def correct_spelling(tokens): \n",
        "    #print(tokens)\n",
        "    spell = SpellChecker()\n",
        "    spellchecked_list = []\n",
        "    # find those words that may be misspelled\n",
        "    word_tokens = word_tokenize(tokens) \n",
        "    for w in word_tokens:\n",
        "      alist = []\n",
        "      alist.append(w)\n",
        "      if len(spell.unknown(alist)) == 0:\n",
        "         spellchecked_list.append(w)\n",
        "      else:\n",
        "         spellchecked_list.append(spell.correction(w))\n",
        "    #print(spellchecked_list)\n",
        "    return spellchecked_list\n",
        "    \n",
        "def wikisuggestion(token):\n",
        "    result = []\n",
        "    word_tokens = word_tokenize(token) \n",
        "    for w in word_tokens:      \n",
        "        spell = SpellChecker()\n",
        "        wiki_list = wikipedia.search(w)\n",
        "        if len(wiki_list) == 0:  # No Suggested Word from Wiki, Correct Spelling with Python Spelling Checker\n",
        "            result.append(spell.correction(w))\n",
        "        else:\n",
        "            for wl in wiki_list:\n",
        "                if wl in data: # Suggested the closest word based on the context. \n",
        "                  result.append(wl)\n",
        "                  break\n",
        "                else:\n",
        "                  result.append(spell.correction(w))\n",
        "                  break\n",
        "    return result\n",
        "\n",
        "def extract_Noun(text):\n",
        "  blob = TextBlob(text)\n",
        "  return blob.noun_phrases\n",
        "\n",
        "def dot_replace(text):\n",
        "  return text.replace(\".\", \" \")\n",
        "\n",
        "def process_all(text):\n",
        "    text = process_text(text)\n",
        "    text = ' '.join(remove_stopwords(stop_words, text.split()))\n",
        "    #text = ' '.join(camel_case_split(text))\n",
        "    #text = ' '.join(correct_spelling(text))\n",
        "    text = ' '.join(extract_Noun(text))\n",
        "    text = ' '.join(stemm(text))\n",
        "    text = ' '.join(lemmatize(text))  \n",
        "    return text\n",
        "\n",
        "def unique_words(sentence):\n",
        "    return set(sentence.lower().split())\n",
        "\n",
        "def feature_names(data):\n",
        "    uniquewords= []\n",
        "    for s in data: \n",
        "       words = unique_words(s)\n",
        "       for w in words:\n",
        "         if w not in uniquewords: \n",
        "           uniquewords.append(w)\n",
        "    return uniquewords\n",
        "\n",
        "def estimate_clusters(data):\n",
        "   #totalyear = len(pd.to_datetime(data['CreatedDate']).dt.strftime(\"%y\").drop_duplicates().tolist())\n",
        "   totalissuescount = len(data)\n",
        "   #value = (totalissuescount/totalyear)/12\n",
        "   value = (totalissuescount/2)\n",
        "   if(value > 1): \n",
        "     return math.sqrt(value)\n",
        "   else: \n",
        "     return 2\n",
        "\n",
        "def evaluate_clusters(cluster_result_df):\n",
        "   TP = 0\n",
        "   TN = 0\n",
        "   FP = 0\n",
        "   FN = 0\n",
        "   for ind in cluster_result_df.index:\n",
        "      filter1 = cluster_result_df['PullRequestID'] == cluster_result_df['PullRequestID'][ind] #Fixed By Same Pull Request (Class)\n",
        "      filter2 = cluster_result_df['IssueID'] != cluster_result_df['IssueID'][ind] #Different Issue\n",
        "      filter3 = cluster_result_df['Cluster'] != cluster_result_df['Cluster'][ind] #Different Cluster\n",
        "      filter4 = cluster_result_df['Cluster'] == cluster_result_df['Cluster'][ind] #Same Cluster\n",
        "      filter5 = cluster_result_df['PullRequestID'] != cluster_result_df['PullRequestID'][ind] #Fixed By Different Pull Request (Class)\n",
        "      filter6 = cluster_result_df['FixedByID'] != cluster_result_df['FixedByID'][ind] #Fixed By Same Developer (Class)\n",
        "      similarIssues = cluster_result_df.where(filter1 & filter2)\n",
        "      if(len(similarIssues.dropna()) > 0):    \n",
        "        if(len(cluster_result_df.where(filter2 & filter4 & filter5 & filter6).dropna()) > 0): #Get List of Issues on the same cluster with different Pull Request     \n",
        "              FP = FP + 1  \n",
        "              print(\"Cluster: \", cluster_result_df['Cluster'][ind], \"PullRequestID: \", cluster_result_df['PullRequestID'][ind],\"Issue: \", cluster_result_df['IssueID'][ind], \"Two : FP\")\n",
        "        elif(len(cluster_result_df.where(filter1 & filter2 & filter3).dropna()) > 0 ): #Get List of Issue with Same Pull Requests (Class) but Different Cluster \n",
        "              FN = FN + 1  \n",
        "              print(\"Cluster: \", cluster_result_df['Cluster'][ind], \"PullRequestID: \", cluster_result_df['PullRequestID'][ind],\"Issue: \", cluster_result_df['IssueID'][ind], \"Two : FN\" )\n",
        "        elif(len(cluster_result_df.where(filter4 & filter5 & filter6).dropna()) == 0): #Get the List issues same cluster with Different Pull Request\n",
        "              TP = TP + 1 \n",
        "              print(\"Cluster: \", cluster_result_df['Cluster'][ind], \"PullRequestID: \", cluster_result_df['PullRequestID'][ind],\"Issue: \", cluster_result_df['IssueID'][ind], \"Two: TP\" )  \n",
        "        else:\n",
        "              TN = TN + 1 \n",
        "              print(\"Cluster: \", cluster_result_df['Cluster'][ind], \"PullRequestID: \", cluster_result_df['PullRequestID'][ind],\"Issue: \", cluster_result_df['IssueID'][ind], \"Two : TN\")       \n",
        "      else: \n",
        "              TP = TP + 1 # Single Issue Fixed By Pull Request  \n",
        "              print(\"Cluster: \", cluster_result_df['Cluster'][ind], \"PullRequestID: \", cluster_result_df['PullRequestID'][ind],\"Issue: \", cluster_result_df['IssueID'][ind], \"Two: TP\") \n",
        "   print(\"Total :\", \"TP :\", TP, \"TN :\", TN, \"FP :\", FP, \"FN :\", FN) \n",
        "   return TP,FP,FN\n",
        "\n",
        "def calculate_results(true_positive, false_positive, false_negative):\n",
        "    if true_positive + false_positive > 0:\n",
        "        precision = true_positive / (true_positive + false_positive)\n",
        "    else:\n",
        "        precision = 0\n",
        "    if true_positive + false_negative > 0:\n",
        "       recall = true_positive / (true_positive + false_negative)\n",
        "    else:\n",
        "         recall = 0\n",
        "    if precision + recall > 0:\n",
        "         f1 = 2 * precision * recall / (precision + recall)\n",
        "    else:\n",
        "        f1 = 0\n",
        "    return precision, recall, f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuYr9wku91AQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_processed = list(map(process_all, list(data['Title'])))\n",
        "BASE_VECTORS = get_features(data_processed)\n",
        "\n",
        "#true_k = int(estimate_clusters(data)) \n",
        "true_k = len(data[\"PullRequestID\"].unique())\n",
        "model = KMeans(n_clusters=true_k, init='random', max_iter=true_k*2, n_init=1)\n",
        "model.fit(BASE_VECTORS)\n",
        "\n",
        "\n",
        "print(\"Top terms per cluster:\")\n",
        "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = feature_names(data_processed)\n",
        "\n",
        "#print assign vector\n",
        "i = 0\n",
        "cluster_result = []\n",
        "for l in model.labels_:\n",
        "    #print(l, \" : \", data_processed[i], data['PullRequestID'][i])  \n",
        "    newrow = {'Cluster': l ,'PullRequestID': data['PullRequestID'][i],'IssueID': data['IssueID'][i],'FixedByID': data['FixedByID'][i] }  \n",
        "    cluster_result.append(newrow) \n",
        "   \n",
        "    #append row to the dataframe\n",
        "    i = i +  1\n",
        "\n",
        "cluster_result_df = pd.DataFrame(cluster_result)\n",
        "\n",
        "### Save Cluster Result to CSV\n",
        "cluster_result_df.to_csv('out.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLJiYFydwZTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " filter1 = cluster_result_df['PullRequestID'] == cluster_result_df['PullRequestID'][1] #Fixed By Same Pull Request (Class)\n",
        " filter2 = cluster_result_df['IssueID'] != cluster_result_df['IssueID'][1] #Different Issue\n",
        " filter3 = cluster_result_df['Cluster'] != cluster_result_df['Cluster'][1] #Different Cluster\n",
        " filter4 = cluster_result_df['Cluster'] == cluster_result_df['Cluster'][1] #Same Cluster\n",
        " filter5 = cluster_result_df['PullRequestID'] != cluster_result_df['PullRequestID'][1] #Fixed By Different Pull Request (Class)\n",
        " filter6 = cluster_result_df['FixedByID'] != cluster_result_df['FixedByID'][1] #Fixed By Same Developer (Class)\n",
        " #len(cluster_result_df.where(filter2 & filter4).dropna())\n",
        " #len(cluster_result_df.where(filter2 & filter4 & filter5 & filter6).dropna())\n",
        " cluster_result_df.where(filter2 & filter4 & (filter5 |filter6)).dropna()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRw7WkfH5rtO",
        "colab_type": "code",
        "outputId": "41d6c42a-5aee-4cc6-aba0-9a05acc7c251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "source": [
        "### Evaluate cluster \n",
        "TP, TN, FN = evaluate_clusters(cluster_result_df)\n",
        "P, R, F1 = calculate_results(TP, TN, FN)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cluster:  23 PullRequestID:  18836 Issue:  16741 Two: TP\n",
            "Cluster:  5 PullRequestID:  18539 Issue:  18467 Two : FP\n",
            "Cluster:  2 PullRequestID:  18539 Issue:  16648 Two : FP\n",
            "Cluster:  10 PullRequestID:  18529 Issue:  18455 Two : FP\n",
            "Cluster:  10 PullRequestID:  18529 Issue:  18496 Two : FP\n",
            "Cluster:  2 PullRequestID:  18459 Issue:  18388 Two : FP\n",
            "Cluster:  26 PullRequestID:  18459 Issue:  18457 Two : FN\n",
            "Cluster:  2 PullRequestID:  18458 Issue:  18388 Two : FP\n",
            "Cluster:  26 PullRequestID:  18458 Issue:  18457 Two : FN\n",
            "Cluster:  22 PullRequestID:  18390 Issue:  18180 Two : FN\n",
            "Cluster:  16 PullRequestID:  18390 Issue:  18385 Two : FN\n",
            "Cluster:  25 PullRequestID:  16309 Issue:  5 Two: TP\n",
            "Cluster:  24 PullRequestID:  15043 Issue:  14970 Two : FP\n",
            "Cluster:  21 PullRequestID:  15043 Issue:  12577 Two : FN\n",
            "Cluster:  11 PullRequestID:  14969 Issue:  7377 Two : FP\n",
            "Cluster:  12 PullRequestID:  14969 Issue:  14773 Two : FP\n",
            "Cluster:  1 PullRequestID:  14967 Issue:  13901 Two : FP\n",
            "Cluster:  24 PullRequestID:  14967 Issue:  14959 Two : FP\n",
            "Cluster:  24 PullRequestID:  14967 Issue:  14960 Two : FP\n",
            "Cluster:  24 PullRequestID:  14967 Issue:  14961 Two : FP\n",
            "Cluster:  24 PullRequestID:  14967 Issue:  14963 Two : FP\n",
            "Cluster:  10 PullRequestID:  14115 Issue:  13969 Two : FP\n",
            "Cluster:  5 PullRequestID:  14115 Issue:  14018 Two : FP\n",
            "Cluster:  10 PullRequestID:  14114 Issue:  13969 Two : FP\n",
            "Cluster:  5 PullRequestID:  14114 Issue:  14018 Two : FP\n",
            "Cluster:  8 PullRequestID:  14063 Issue:  14056 Two : FP\n",
            "Cluster:  7 PullRequestID:  14063 Issue:  14050 Two : FN\n",
            "Cluster:  19 PullRequestID:  14007 Issue:  13162 Two : FN\n",
            "Cluster:  9 PullRequestID:  14007 Issue:  13991 Two : FP\n",
            "Cluster:  19 PullRequestID:  14006 Issue:  13162 Two : FN\n",
            "Cluster:  9 PullRequestID:  14006 Issue:  13991 Two : FP\n",
            "Cluster:  15 PullRequestID:  11539 Issue:  11442 Two: TP\n",
            "Cluster:  15 PullRequestID:  11539 Issue:  11441 Two: TP\n",
            "Cluster:  14 PullRequestID:  11506 Issue:  11505 Two : FP\n",
            "Cluster:  6 PullRequestID:  11506 Issue:  11478 Two : FN\n",
            "Cluster:  4 PullRequestID:  11449 Issue:  11437 Two : FN\n",
            "Cluster:  0 PullRequestID:  11449 Issue:  11434 Two : FP\n",
            "Cluster:  4 PullRequestID:  11448 Issue:  11437 Two : FN\n",
            "Cluster:  0 PullRequestID:  11448 Issue:  11434 Two : FP\n",
            "Cluster:  20 PullRequestID:  7914 Issue:  7907 Two : FN\n",
            "Cluster:  1 PullRequestID:  7914 Issue:  7657 Two : FP\n",
            "Cluster:  20 PullRequestID:  7913 Issue:  7907 Two : FN\n",
            "Cluster:  1 PullRequestID:  7913 Issue:  7657 Two : FP\n",
            "Cluster:  10 PullRequestID:  7863 Issue:  4437 Two : FP\n",
            "Cluster:  11 PullRequestID:  7863 Issue:  13767 Two : FP\n",
            "Cluster:  9 PullRequestID:  961 Issue:  18118 Two : FP\n",
            "Cluster:  14 PullRequestID:  961 Issue:  18114 Two : FP\n",
            "Cluster:  18 PullRequestID:  961 Issue:  18113 Two : FP\n",
            "Cluster:  13 PullRequestID:  948 Issue:  2394 Two: TP\n",
            "Cluster:  10 PullRequestID:  942 Issue:  2090 Two : FP\n",
            "Cluster:  12 PullRequestID:  942 Issue:  15504 Two : FP\n",
            "Cluster:  3 PullRequestID:  942 Issue:  16374 Two : FN\n",
            "Cluster:  18 PullRequestID:  701 Issue:  11778 Two : FP\n",
            "Cluster:  8 PullRequestID:  701 Issue:  9611 Two : FP\n",
            "Cluster:  23 PullRequestID:  584 Issue:  10789 Two : FP\n",
            "Cluster:  0 PullRequestID:  584 Issue:  6404 Two : FP\n",
            "Total : TP : 5 TN : 0 FP : 37 FN : 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KjW3Okie80M",
        "colab_type": "code",
        "outputId": "0e8cc011-12a8-48f8-9261-39c186b69059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(P, R, F1)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.11904761904761904 0.2631578947368421 0.1639344262295082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp71fR5CecCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_clusters(cluster_result_df):\n",
        "   TP = 0\n",
        "   TN = 0\n",
        "   FP = 0\n",
        "   FN = 0\n",
        "   for ind in cluster_result_df.index:\n",
        "      filter1 = cluster_result_df['PullRequestID'] == cluster_result_df['PullRequestID'][ind] #Fixed By Same Pull Request (Class)\n",
        "      filter2 = cluster_result_df['IssueID'] != cluster_result_df['IssueID'][ind] #Different Issue\n",
        "      filter3 = cluster_result_df['Cluster'] != cluster_result_df['Cluster'][ind] #Different Cluster\n",
        "      filter4 = cluster_result_df['Cluster'] == cluster_result_df['Cluster'][ind] #Same Cluster\n",
        "      filter5 = cluster_result_df['PullRequestID'] != cluster_result_df['PullRequestID'][ind] #Fixed By Different Pull Request (Class)\n",
        "      filter6 = cluster_result_df['FixedByID'] != cluster_result_df['FixedByID'][ind] #Fixed By Same Developer (Class)\n",
        "      similarIssues = cluster_result_df.where(filter1 & filter2)\n",
        "      if(len(similarIssues.dropna()) > 0):    \n",
        "        if(len(cluster_result_df.where(filter2 & filter4 & filter5 & filter6).dropna()) > 0): #Get List of Issues on the same cluster with different Pull Request     \n",
        "              FP = FP + 1  \n",
        "              print(\"Cluster: \", cluster_result_df['Cluster'][ind], \"PullRequestID: \", cluster_result_df['PullRequestID'][ind],\"Issue: \", cluster_result_df['IssueID'][ind], \"Two : FP\")\n",
        "        elif(len(cluster_result_df.where(filter1 & filter2 & filter3).dropna()) > 0 ): #Get List of Issue with Same Pull Requests (Class) but Different Cluster \n",
        "              FN = FN + 1  \n",
        "              print(\"Cluster: \", cluster_result_df['Cluster'][ind], \"PullRequestID: \", cluster_result_df['PullRequestID'][ind],\"Issue: \", cluster_result_df['IssueID'][ind], \"Two : FN\" )\n",
        "        elif(len(cluster_result_df.where(filter4 & filter5 & filter6).dropna()) == 0): #Get the List issues same cluster with Different Pull Request\n",
        "              TP = TP + 1 \n",
        "              print(\"Cluster: \", cluster_result_df['Cluster'][ind], \"PullRequestID: \", cluster_result_df['PullRequestID'][ind],\"Issue: \", cluster_result_df['IssueID'][ind], \"Two: TP\" )  \n",
        "        else:\n",
        "              TN = TN + 1 \n",
        "              print(\"Cluster: \", cluster_result_df['Cluster'][ind], \"PullRequestID: \", cluster_result_df['PullRequestID'][ind],\"Issue: \", cluster_result_df['IssueID'][ind], \"Two : TN\")       \n",
        "      else: \n",
        "              TP = TP + 1 # Single Issue Fixed By Pull Request  \n",
        "              print(\"Cluster: \", cluster_result_df['Cluster'][ind], \"PullRequestID: \", cluster_result_df['PullRequestID'][ind],\"Issue: \", cluster_result_df['IssueID'][ind], \"Two: TP\") \n",
        "   print(\"Total :\", \"TP :\", TP, \"TN :\", TN, \"FP :\", FP, \"FN :\", FN) \n",
        "   return TP,FP,FN\n",
        "\n",
        "def calculate_results(true_positive, false_positive, false_negative):\n",
        "    if true_positive + false_positive > 0:\n",
        "        precision = true_positive / (true_positive + false_positive)\n",
        "    else:\n",
        "        precision = 0\n",
        "    if true_positive + false_negative > 0:\n",
        "       recall = true_positive / (true_positive + false_negative)\n",
        "    else:\n",
        "         recall = 0\n",
        "    if precision + recall > 0:\n",
        "         f1 = 2 * precision * recall / (precision + recall)\n",
        "    else:\n",
        "        f1 = 0\n",
        "    return precision, recall, f1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}