{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "K_Mean_With_Sentence_Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJrBFiVd1PT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install wikipedia"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7ARYdkb1v5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install pyspellchecker  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6TTVEcL15FY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import nltk\n",
        "import re\n",
        "import wikipedia\n",
        "import pandas as pd\n",
        "import datetime\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from string import punctuation\n",
        "from nltk.stem import PorterStemmer\n",
        "from spellchecker import SpellChecker\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "data = pd.read_csv('IssueRoslyn.csv', encoding='latin-1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVpOxAsl2Y0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tensroflow hub module for Universal sentence Encoder \n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" \n",
        "embed = hub.Module(module_url)\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def get_features(texts):\n",
        "    if type(texts) is str:\n",
        "        texts = [texts]\n",
        "    with tf.Session() as sess:\n",
        "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "        return sess.run(embed(texts))\n",
        "\n",
        "def remove_stopwords(stop_words, tokens):\n",
        "    res = []\n",
        "    for token in tokens:\n",
        "        if not token in stop_words:\n",
        "            res.append(token)\n",
        "    return res\n",
        "\n",
        "def process_text(text):\n",
        "    text = text.encode('ascii', errors='ignore').decode()\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', ' ', text)\n",
        "    text = re.sub(r'#+', ' ', text )\n",
        "    text = text.strip() #Remove white space from beginning and ending\n",
        "    return text\n",
        "\n",
        "def camel_case_split(tokens): \n",
        "   words = []\n",
        "   word_tokens = word_tokenize(tokens)\n",
        "   for token in word_tokens:\n",
        "        words = [[token[0]]]   \n",
        "        for c in tokens[1:]: \n",
        "            if words[-1][-1].islower() and c.isupper(): \n",
        "                words.append(list(c)) \n",
        "            else: \n",
        "                words[-1].append(c)   \n",
        "   return words\n",
        "\n",
        "def lemmatize(tokens):\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    lemma_list = []\n",
        "    word_tokens = word_tokenize(tokens) \n",
        "    for token in word_tokens:\n",
        "        lemma = lemmatizer.lemmatize(token, 'v')\n",
        "        if lemma == token:\n",
        "            lemma = lemmatizer.lemmatize(token)\n",
        "        lemma_list.append(lemma)  \n",
        "    return lemma_list\n",
        "\n",
        "def stemm(tokens):  \n",
        "  ps = PorterStemmer()\n",
        "  stem_list = []\n",
        "  word_tokens = word_tokenize(tokens) \n",
        "  for w in word_tokens:\n",
        "      rootWord = ps.stem(w)\n",
        "      stem_list.append(rootWord)\n",
        "  return stem_list\n",
        "\n",
        "def correct_spelling(tokens): \n",
        "    #print(tokens)\n",
        "    spell = SpellChecker()\n",
        "    spellchecked_list = []\n",
        "    # find those words that may be misspelled\n",
        "    word_tokens = word_tokenize(tokens) \n",
        "    for w in word_tokens:\n",
        "      alist = []\n",
        "      alist.append(w)\n",
        "      if len(spell.unknown(alist)) == 0:\n",
        "         spellchecked_list.append(w)\n",
        "      else:\n",
        "         spellchecked_list.append(wikisuggestion(w))\n",
        "    #print(spellchecked_list)\n",
        "    return spellchecked_list\n",
        "\n",
        "def wikisuggestion(token):\n",
        "    spell = SpellChecker()\n",
        "    wiki_list = wikipedia.search(token)\n",
        "    if len(wiki_list) == 0:  # No Suggested Word from Wiki, Correct Spelling with Python Spelling Checker\n",
        "        return spell.correction(token)\n",
        "    else:\n",
        "        for wl in wiki_list:\n",
        "           if wl in data: # Suggested the closest word based on the context. \n",
        "              return wl\n",
        "           else:\n",
        "              result = wikipedia.search(wl)[0]\n",
        "              result = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", result)\n",
        "              return result\n",
        "   \n",
        "def process_all(text):\n",
        "    text = process_text(text)\n",
        "    #text = ' '.join(remove_stopwords(stop_words, text.split()))\n",
        "    #text = ' '.join(camel_case_split(text))\n",
        "    text = ' '.join(correct_spelling(text))\n",
        "    #text = ' '.join(stemm(text))\n",
        "    #text = ' '.join(lemmatize(text))    \n",
        "    return text\n",
        "\n",
        "def unique_words(sentence):\n",
        "    return set(sentence.lower().split())\n",
        "\n",
        "def feature_names(data):\n",
        "    uniquewords= []\n",
        "    for s in data: \n",
        "       words = unique_words(s)\n",
        "       for w in words:\n",
        "         if w not in uniquewords: \n",
        "           uniquewords.append(w)\n",
        "    return uniquewords\n",
        "\n",
        "def estimate_clusters(data):\n",
        "   totalyear = len(pd.to_datetime(data['CreatedDate']).dt.strftime(\"%y\").drop_duplicates().tolist())\n",
        "   totalissuescount = len(data)\n",
        "   return (totalissuescount/totalyear)/12\n",
        "\n",
        "data_processed = list(map(process_all, list(data['Description'])))\n",
        "BASE_VECTORS = get_features(data_processed)\n",
        "\n",
        "true_k = int(estimate_clusters(data))\n",
        "model = KMeans(n_clusters=true_k, init='random', max_iter=2, n_init=1)\n",
        "model.fit(BASE_VECTORS)\n",
        "\n",
        "\n",
        "print(\"Top terms per cluster:\")\n",
        "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = feature_names(data_processed)\n",
        "\n",
        "#print assign vector\n",
        "i = 0\n",
        "cluster_result_df = pd.DataFrame(columns=['Cluster', 'PullRequestID', 'IssueID'])\n",
        "\n",
        "for l in model.labels_:\n",
        "    print(l, \" : \", data_processed[i], data['PullRequestID'][i])  \n",
        "    new_row = {'Cluster' : l , 'PullRequestID' : data['PullRequestID'][i], 'IssueID' : data['IssueID'][i]}\n",
        "     #append row to the dataframe\n",
        "    cluster_result_df = cluster_result_df.append(new_row, ignore_index=True)\n",
        "    i = i +  1\n",
        "\n",
        "### Save Cluster Result to CSV\n",
        "cluster_result_df.to_csv('out.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}