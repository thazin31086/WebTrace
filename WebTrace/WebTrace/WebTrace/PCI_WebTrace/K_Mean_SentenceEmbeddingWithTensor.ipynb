{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "K_Mean_SentenceEmbeddingWithTensor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph8WYNElFylM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import nltk\n",
        "import re\n",
        "import wikipedia\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from string import punctuation\n",
        "from nltk.stem import PorterStemmer\n",
        "from spellchecker import SpellChecker\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "\n",
        "data = [\n",
        "    \"Visual Studio 2019 crashing when click RMB on rule in Analyzers' dependencies\",#https://github.com/dotnet/roslyn/issues/40720\n",
        "    \"Avoid crash on concat on structs with ToString member\", #https://github.com/dotnet/roslyn/pull/38860/commits\n",
        "    \"Enum implicit cast to string fails when element is named ToString\", #https://github.com/dotnet/roslyn/issues/40256\n",
        "    \"Enum with ToString member crashes in string concatenation\", #https://github.com/dotnet/roslyn/issues/38858   \n",
        "    \"Crash on right click a Analyze rule in Solution-Explorer\", #https://github.com/dotnet/roslyn/issues/36304\n",
        "    \"Handle lazy loading of analyzer command handlers\", #https://github.com/dotnet/roslyn/pull/36740\n",
        "    ]\n",
        "\n",
        "# tensroflow hub module for Universal sentence Encoder \n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" \n",
        "embed = hub.Module(module_url)\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def get_features(texts):\n",
        "    if type(texts) is str:\n",
        "        texts = [texts]\n",
        "    with tf.Session() as sess:\n",
        "        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "        return sess.run(embed(texts))\n",
        "\n",
        "def remove_stopwords(stop_words, tokens):\n",
        "    res = []\n",
        "    for token in tokens:\n",
        "        if not token in stop_words:\n",
        "            res.append(token)\n",
        "    return res\n",
        "\n",
        "def process_text(text):\n",
        "    text = text.encode('ascii', errors='ignore').decode()\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', ' ', text)\n",
        "    text = re.sub(r'#+', ' ', text )\n",
        "    text = text.strip() #Remove white space from beginning and ending\n",
        "    return text\n",
        "\n",
        "def camel_case_split(tokens): \n",
        "   words = []\n",
        "   word_tokens = word_tokenize(tokens)\n",
        "   for token in word_tokens:\n",
        "        words = [[token[0]]]   \n",
        "        for c in tokens[1:]: \n",
        "            if words[-1][-1].islower() and c.isupper(): \n",
        "                words.append(list(c)) \n",
        "            else: \n",
        "                words[-1].append(c)   \n",
        "   return words\n",
        "\n",
        "def lemmatize(tokens):\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    lemma_list = []\n",
        "    word_tokens = word_tokenize(tokens) \n",
        "    for token in word_tokens:\n",
        "        lemma = lemmatizer.lemmatize(token, 'v')\n",
        "        if lemma == token:\n",
        "            lemma = lemmatizer.lemmatize(token)\n",
        "        lemma_list.append(lemma)  \n",
        "    return lemma_list\n",
        "\n",
        "def stemm(tokens):  \n",
        "  ps = PorterStemmer()\n",
        "  stem_list = []\n",
        "  word_tokens = word_tokenize(tokens) \n",
        "  for w in word_tokens:\n",
        "      rootWord = ps.stem(w)\n",
        "      stem_list.append(rootWord)\n",
        "  return stem_list\n",
        "\n",
        "def correct_spelling(tokens): \n",
        "    #print(tokens)\n",
        "    spell = SpellChecker()\n",
        "    spellchecked_list = []\n",
        "    # find those words that may be misspelled\n",
        "    word_tokens = word_tokenize(tokens) \n",
        "    for w in word_tokens:\n",
        "      alist = []\n",
        "      alist.append(w)\n",
        "      if len(spell.unknown(alist)) == 0:\n",
        "         spellchecked_list.append(w)\n",
        "      else:\n",
        "         spellchecked_list.append(wikisuggestion(w))\n",
        "    #print(spellchecked_list)\n",
        "    return spellchecked_list\n",
        "\n",
        "def wikisuggestion(token):\n",
        "    spell = SpellChecker()\n",
        "    wiki_list = wikipedia.search(token)\n",
        "    if len(wiki_list) == 0:  # No Suggested Word from Wiki, Correct Spelling with Python Spelling Checker\n",
        "        return spell.correction(token)\n",
        "    else:\n",
        "        for wl in wiki_list:\n",
        "           if wl in data: # Suggested the closest word based on the context. \n",
        "              return wl\n",
        "           else:\n",
        "              result = wikipedia.search(wl)[0]\n",
        "              result = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", result)\n",
        "              return result\n",
        "   \n",
        "def process_all(text):\n",
        "    text = process_text(text)\n",
        "    text = ' '.join(remove_stopwords(stop_words, text.split()))\n",
        "    #text = ' '.join(camel_case_split(text))\n",
        "    text = ' '.join(correct_spelling(text))\n",
        "    #text = ' '.join(stemm(text))\n",
        "    #text = ' '.join(lemmatize(text))    \n",
        "    return text\n",
        "\n",
        "def unique_words(sentence):\n",
        "    return set(sentence.lower().split())\n",
        "\n",
        "def feature_names(data):\n",
        "    uniquewords= []\n",
        "    for s in data: \n",
        "       words = unique_words(s)\n",
        "       for w in words:\n",
        "         if w not in uniquewords: \n",
        "           uniquewords.append(w)\n",
        "    return uniquewords\n",
        "\n",
        "\n",
        "data_processed = list(map(process_all, data))\n",
        "BASE_VECTORS = get_features(data_processed)\n",
        "\n",
        "\n",
        "def input_fn():\n",
        "  return tf.train.limit_epochs(\n",
        "      BASE_VECTORS, num_epochs=1)\n",
        "\n",
        "num_clusters = 2\n",
        "cluster_centerlist = []\n",
        "kmeans = tf.estimator.experimental.KMeans(\n",
        "    num_clusters=num_clusters, initial_clusters='random', use_mini_batch=False)\n",
        "# train\n",
        "num_iterations = 10\n",
        "previous_centers = None\n",
        "for _ in range(num_iterations):\n",
        "  \n",
        "  kmeans.train(input_fn)\n",
        "  cluster_centers = kmeans.cluster_centers()\n",
        "  #if previous_centers is not None:\n",
        "    #print('delta:', cluster_centers - previous_centers)\n",
        "    #print('delta:', cluster_centers - previous_centers)\n",
        "  previous_centers = cluster_centers\n",
        "  cluster_centerlist.append(cluster_centers)\n",
        "  #print('score:', kmeans.score(input_fn))\n",
        "#print('cluster centers:', cluster_centers)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8lZRx_uO9Bq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "4e4610bd-23db-424c-a9c6-f8b1d9775068"
      },
      "source": [
        "print(cluster_centerlist[0])\n",
        "print(cluster_centerlist[1])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.04058949 -0.04058962  0.0336156  ...  0.04999132  0.0534329\n",
            "  -0.01757501]\n",
            " [ 0.01112611 -0.04777005  0.08043283 ...  0.08039761  0.03552148\n",
            "  -0.03916642]]\n",
            "[[ 0.03608557 -0.04016418  0.0241436  ...  0.06572434  0.05265355\n",
            "  -0.03146098]\n",
            " [ 0.01434285 -0.0173102   0.05734048 ...  0.04390229 -0.00920612\n",
            "   0.01444885]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwtvbkTsQOfB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "bf44b11a-59a2-41e5-de07-d117adb35afc"
      },
      "source": [
        "cluster_indices = list(kmeans.predict_cluster_index(input_fn))\n",
        "print(cluster_indices)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /tmp/tmpq_muisz7/model.ckpt-19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /tmp/tmpq_muisz7/model.ckpt-19\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 0, 0, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTPESiyxLpPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# map the input points to their clusters\n",
        "cluster_indices = list(kmeans.predict_cluster_index(input_fn))\n",
        "for i, point in enumerate(BASE_VECTORS):\n",
        "  cluster_index = cluster_indices[i]\n",
        "  center = cluster_centers[cluster_index]\n",
        "  print('point:', point, 'is in cluster', cluster_index, 'centered at', center)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssRMYcpJKxuW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24471297-2c4b-4014-9cf2-21e3c8135415"
      },
      "source": [
        "cluster_indices"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 0, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}